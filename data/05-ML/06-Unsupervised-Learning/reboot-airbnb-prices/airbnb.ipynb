{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Machine Learning Reboot Challenge üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ready to apply your knowledge in a hands-on experience with Airbnb data.\n",
    "\n",
    "We'll work through:\n",
    "\n",
    "üßπ Data Cleaning\n",
    "\n",
    "üö¶ Train-Test Split\n",
    "\n",
    "üë∑ Preprocessing the proper way with pipelines\n",
    "\n",
    "üìà Linear Regression \n",
    "\n",
    "üîÅ Cross-Validation\n",
    "\n",
    "üéØ Fine-Tuning\n",
    "\n",
    "Let's dive in and bring these concepts to life! üèä‚Äç‚ôÄÔ∏èüèä‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understand the data üëè and basic cleaning üßπ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, we will use real Airbnb data. This data comes from many cities. We'll want to make our code easy to reuse. This way, we can work with different CSV files quickly, because we won't need to write new code for each csv!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up: ***Asheville, North Carolina***. The CSV file is available here: [üîó link üîó](https://wagon-public-datasets.s3.amazonaws.com/data-science-images/05-ML/Reboot-2/asheville_airbnb.csv).\n",
    "\n",
    "We're going to use a linear regression to predict the price of an Airbnb accomodation based on all the information we have about it!\n",
    "\n",
    "üëâ First, let's load the CSV file into a DataFrame called `df`.\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Hint</i></summary>\n",
    "\n",
    "The first (unnamed) column in the CSV is actually an index column. That's the index that Pandas is creating by default, but it's not so helpful when we already have one!\n",
    "    \n",
    "To get rid of it, either use the `.reset_index()` method, or - **easier!** - use `index_col=0` when reading the CSV file.\n",
    "\n",
    "Think about what index_col=0 is doing. Could we use `index_col` if our existing index was in the 3rd column?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/05-ML/Reboot-2/asheville_airbnb.csv\"\n",
    "\n",
    "# Load the data into `df`\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Investigate the dtypes of your DataFrame. Something is wrong. Do you see it?\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "\n",
    "The `price` column is an object, not a float.\n",
    "\n",
    "</details>\n",
    "\n",
    "üëâ Clean the column you identified.\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Hint</i></summary>\n",
    "\n",
    "You'll need the [`.str.replace()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html) and the [`.astype()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.astype.html) methods. \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('df',\n",
    "    df_columns=df.columns,\n",
    "    price_dtype=df['price'].dtype\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Take a look at the distribution of the target. Also check out its minimum and maximum values.\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Hint</i></summary>\n",
    "\n",
    "Use the `.hist()` method of a pd.Series. You can apply it to your target column. \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks quite skewed! We're going to focus on Airbnb listings priced between 50 and 1500 dollars (both included).\n",
    "\n",
    "üëâ Create a DataFrame named `reduced` with only listings priced between 50 and 1500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('reduced',\n",
    "    reduced_min=reduced['price'].min(),\n",
    "    reduced_max=reduced['price'].max(),\n",
    "    reduced_shape=reduced.shape\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Inspect your `reduced` DataFrame. \n",
    "\n",
    "There are two columns that we will need to change a bit before we start our modeling. Do you see which ones and why?\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "\n",
    "- The `bathrooms_text` column seems to have mainly numeric information, but in a text format.\n",
    "- The `instant_bookable` column has `t` and `f` values. It would be easier for us to have just 1s and 0s instead.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the first problematic column. \n",
    "\n",
    "üëâ Have a look at the unique values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the cleaning function (a little üéÅ for you), but you will have to _apply_ it to the DataFrame yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def extract_number(text):\n",
    "    if text.lower() == \"half-bath\":\n",
    "        return 0.5\n",
    "    else:\n",
    "        return float(text.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Use the function to clean the column, and save the result back in the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's work on the other column. There are multiple ways to transform the values into 0s and 1s. \n",
    "\n",
    "üëâ Pick the one you're comfortable with (as long as it's not a for loop...) and use it to clean the column.\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Hint</i></summary>\n",
    "    \n",
    "- [`pandas.Series.map()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html) with a mapping dictionary\n",
    "- [`pandas.Series.apply()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) with a lambda function (not recommended on large datasets because it is slow)\n",
    "- [`numpy.where()`](https://numpy.org/doc/stable/reference/generated/numpy.where.html)\n",
    "- Boolean indexing\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('cleaned',\n",
    "    reduced_shape=reduced.shape,\n",
    "    bathrooms_text_dtype=reduced['bathrooms_text'].dtype,\n",
    "    instant_bookable_dtype=reduced['instant_bookable'].dtype,\n",
    "\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a reusable function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have done all the cleaning. But what if later we want to do this for another CSV file?\n",
    "\n",
    "üëâ Create a `df_load_and_clean` function that combines all the previous steps. If all goes well, you should not be doing much more than just copy-pasting from what you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_load_and_clean(url):\n",
    "    '''Loads a CSV from the `url` provide and does the basic cleaning\n",
    "    '''\n",
    "    # Read the CSV into a dataframe\n",
    "    \n",
    "\n",
    "    # Clean and convert price into a float\n",
    "    \n",
    "\n",
    "    # Keep only prices between 50 and 1500\n",
    "    \n",
    "\n",
    "    # Clean and convert bathrooms_text into a float\n",
    "    \n",
    "\n",
    "    # Convert instant_bookable into 0s and 1s\n",
    "    \n",
    "\n",
    "    # Return the result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_load_and_clean(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('load_and_clean',\n",
    "    new_df=new_df,\n",
    "    new_shape=new_df.shape,\n",
    "    price_dtype=new_df['price'].dtype,\n",
    "    bathrooms_text_dtype=new_df['bathrooms_text'].dtype,\n",
    "    instant_bookable_dtype=new_df['instant_bookable'].dtype\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You now have a function that loads the data for you, and does the cleaning for you. Time to go to the next step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split üö¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the relevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the columns and pick only the ones that can help us with our linear regression task. \n",
    "\n",
    "Hint: some columns really won't have added value for our regression task. Drop those.\n",
    "\n",
    "üëâ Create `X` and `y` starting from `new_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('x_and_y',\n",
    "    x_shape=X.shape,\n",
    "    y_shape=y.shape,\n",
    "    x_columns=X.columns\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Split your data in a train and a test set.\n",
    "\n",
    "Let's do a 80/20 split.\n",
    "\n",
    "And let's use a random state of `42`. (In your final pipeline you'd want to remove the random state, but here it will help us to compare our results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing üë∑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already did some basic cleaning, but we'll need some preprocessing steps. Let's do those using `scikit-learn`. We will immediately do our preprocessing using pipelines.\n",
    "\n",
    "‚ùì Why do we want to use pipelines?\n",
    "\n",
    "<details>\n",
    "  <summary><i>Answer</i>\n",
    "  </summary>\n",
    "\n",
    "  1. They allow us to structure our preprocessing nicely\n",
    "  2. They make it easy to fit our preprocessing to the training set and to transform the training set, the test set, and any new data we want to feed into our model\n",
    "  3. They will allow us to properly cross-validate and grid search a full pipeline with preprocessing and model\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default preprocessors output NumPy arrays. We prefer to output Pandas DataFrames because they are easier to work with.\n",
    "\n",
    "You can use the `.set_output(transform='pandas')` method of your transformer to keep the output in a DataFrame format instead of a numpy array.\n",
    "\n",
    "üí°¬†But instead of doing that for each transformer, you can also set this globally. Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Create a list with the numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: print your columns and then copy-paste them to form your new list\n",
    "# It's easier to remove columns from a list than to type (and fix typos)\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [] # YOUR LIST HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Examine the missing values. How many columns have missing values? How many values are missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to do something about these missing values. Which strategies can you apply?\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "\n",
    "- **Drop the columns** with missing values, but we only have a small percentage of missing values. So let's not do that.\n",
    "- **Drop the rows** with missing values. Quick and dirty, but for most columns we actually have the values. So let's not do that.\n",
    "- **Impute missing values**: fill in the blanks.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the columns with missing values. What would be the best imputation strategy for each? Check each column's unique values to make up your mind.\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "\n",
    "- `bedrooms`: if there is no data, we could assume there is no separate bedroom. So impute with 0s.\n",
    "- `beds`: if there is no data, we could assume there are no beds. So impute with 0s.\n",
    "- `review_scores_rating`: if there is no data, we could assume that there is no rating yet, and we could impute with the average rating. (You could make a case for other strategies in this case. A [`KNNImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) could be interesting here, but we'll stick to our simple imputing strategy for now.)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Let's prepare our imputers.\n",
    "\n",
    "Because we have two different strategies, you'll need two separate imputers: one for the 0 imputation, and one for the mean imputation!\n",
    "\n",
    "Name them `zero_imputer` and `mean_imputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Instantiate your zero_imputer and mean_imputer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Make two lists zero_imputer_cols and mean_imputer_cols\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Each list should contain the column names that will be zero / mean imputed\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# This will come in handy later when we will create our pipeline\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can combine our two imputers in a column transformer as a first step in our pipeline.\n",
    "\n",
    "üëâ Make a column transformer called `num_imputer` to :\n",
    "- zero impute the columns we defined above\n",
    "- mean impute all other columns unchanged (hint: think of the `remainder` parameter)\n",
    "\n",
    "(The other columns didn't have missing values, but we might use our pipeline for other cities too. That's why we'll mean impute all other columns for now.)\n",
    "\n",
    "You can choose between using the class based approach (`Pipeline`, `ColumnTransformer`) or the `make_pipeline`, `make_column_transformer` shortcut functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "# Create num_imputer\n",
    "pass  # YOUR CODE HERE\n",
    "\n",
    "# Display num_imputer to check\n",
    "num_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Which columns do you need to scale?\n",
    "\n",
    "Also check the distributions to see which scaler to choose. üëâ Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "clear_output"
    ]
   },
   "outputs": [],
   "source": [
    "X_train[num_cols].hist(figsize=(15,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Instantiate an appropriate scaler. For now, to go fast, we'll scale all our selected columns with a RobustScaler. Later we should come back to this step, and choose the most appropriate scaler for each column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Instantiate your scaler. Call it `scaler`\n",
    "# Pro tip: use the .set_output(transform='pandas') method\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created an imputer for our numerical features, and a scaler. We can now create a pipeline to combine both steps.\n",
    "\n",
    "üëâ Make a pipeline called `num_pipe` to :\n",
    "- impute missing values using our `num_imputer`\n",
    "- scale features using our `scaler`\n",
    "\n",
    "You can choose between using the class based approach (`Pipeline`, `ColumnTransformer`) or the `make_pipeline`, `make_column_transformer` shortcut functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# Create num_pipe:\n",
    "# It should first use your num_imputer and then your scaler\n",
    "pass  # YOUR CODE HERE\n",
    "\n",
    "# Display num_pipe to check your pipeline\n",
    "num_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('scaler',\n",
    "    num_cols=num_cols,\n",
    "    X_train_scaled=num_pipe.fit_transform(X_train[num_cols])\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encode the categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Which columns do you need to encode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [] # YOUR LIST HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good practice: check how many different values you have in each of the columns.\n",
    "\n",
    "üëâ Run the cell below to have a look.\n",
    "\n",
    "Remember that every value will become a column in our X after the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "clear_output"
    ]
   },
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    display(pd.DataFrame(X_train[col].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that for one of these columns we'll have to be a bit smart when encoding. Otherwise we'll have way too many columns in our end result.\n",
    "\n",
    "üëâ Find the right parameter for the OneHotEncoder to limit the number of created columns.\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "There are two candidates:\n",
    "<ul>\n",
    "    <li><strong>max_categories</strong>\n",
    "    </li>\n",
    "    <li><strong>min_frequency</strong>\n",
    "    </li>\n",
    "</ul>\n",
    "    Let's use <code>max_categories=10</code> for our first iteration. We can change this later.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Now create a one hot encoder for your categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Instantiate your OneHotEncoder. Call it `encoder`\n",
    "# Use the .set_output() method again!\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a OneHotEncoder for our categorical features.\n",
    "\n",
    "Because we didn't have any missing values for our categorical features, that was enough for our case. But remember: we might use our pipeline for other cities too. So it's better to add an imputer for missing values.\n",
    "\n",
    "üëâ Make a pipeline called `cat_pipe` to :\n",
    "- impute missing values with the most frequent value\n",
    "- scale features using our `scaler`\n",
    "\n",
    "Use a different approach than you used in your `num_pipe` (class based vs shortcut function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# Create cat_pipe:\n",
    "# It should first apply an imputer and then your scaler\n",
    "pass  # YOUR CODE HERE\n",
    "\n",
    "# Display cat_pipe to check your pipeline\n",
    "cat_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('encoder',\n",
    "    cat_cols=cat_cols,\n",
    "    X_train_encoded=cat_pipe.fit_transform(X_train[cat_cols])\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing numerical and categorical features back together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a pipeline for the numerical and a one-hot-encoder for the categorical features. Let's bring them together.\n",
    "\n",
    "üëâ Make a new column transformer called `preprocessor` that will pipe the numerical features through our `num_pipe`, and the categorical features through our `cat_pipe`.\n",
    "\n",
    "If in the previous step you used the class based approach (`Pipeline`, `ColumnTransformer`), then switch to the `make_pipeline`, `make_column_transformer` shortcut functions this time. Or the other way around. Just make sure you switch üôÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor:\n",
    "# Use num_pipe for the num_cols and cat_pipe for the cat_cols\n",
    "pass  # YOUR CODE HERE\n",
    "\n",
    "# Display preprocessor to check your pipeline\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the difference between using the class based approach and the `make_***` shortcut functions? \n",
    "\n",
    "The class based approach let's you define nice names for the different steps. If you use `make_***`, scikit-learn will determine the name for you. This becomes especially visible when using `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recap what we did:\n",
    "\n",
    "\n",
    "- For the **numerical** columns (our `num_cols`):\n",
    "  - Firstly, we imputed missing values using our imputers:\n",
    "    - `zero_imputer` for features `bedrooms` and `beds`\n",
    "    - `mean_imputer` for features `review_scores_rating`\n",
    "    - The other columns did not have any missing values, but because we are going to use our preprocessor also for other cities, we put all of them also through the `mean_imputer`\n",
    "  - Secondly, we scaled all numerical features with our `scaler`\n",
    "  \n",
    "  \n",
    "-  For the **categorical** columns (our `cat_cols`):\n",
    "   - Impute missing values. We didn't have any in our data, but again we want it to work for other cities too. We impute the most frequent value.\n",
    "   - Encode with our `encoder`\n",
    "  \n",
    "A visual summary of what our pipeline looks like:\n",
    "\n",
    "<img alt=\"Pipeline schema\" src=\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/05-ML/Reboot-2/ml-reboot-pipeline.png\" width=700>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our pipeline to preprocess our train and test sets.\n",
    "\n",
    "Store them in X_train_preprocessed and X_test_preprocessed.\n",
    "\n",
    "You will probably run into a couple of errors. Most probably they come from the OneHotEncoder. Up to you to find the right parameters for your OneHotEncode and change them above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "X_train_preprocessed.shape, X_test_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a UserWarning about unknown categories when preprocessing the test set: that is normal. Do you know why?\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "    Remember our OneHotEncoder?\n",
    "    <br>\n",
    "    It encodes all the categories in the train set into columns with 1s and 0s.\n",
    "    <br>\n",
    "    It's possible that the encoder will encounter categories in the test set that it hasn't seen during the fit. For those rows, all encoded columns will contain 0s (because we chose <code>handle_unknown='ignore'</code>).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('pipeline',\n",
    "    X_train_preprocessed=X_train_preprocessed,\n",
    "    X_test_preprocessed=X_test_preprocessed,\n",
    "    X_train_to_test_pipeline=preprocessor.fit_transform(X_train),\n",
    "    X_test_to_test_pipeline_not_fitted_on_test=preprocessor.transform(X_test),\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ Congratulations! You built your complete preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling üìà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a pipeline to preprocess our data, we can finally get into modelling.\n",
    "\n",
    "We will use the [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#elasticnet) linear regression model, that will give us some parameters to play with in the next phase üéØ. \n",
    "\n",
    "Do you remember what the ElasticNet model is?\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "    ElasticNet is a linear regreession, with a mixture of L1 and L2 regularization. It basically combines Ridge and Lasso regression.\n",
    "</details>\n",
    "\n",
    "üëâ Instantiate an ElasticNet model and call it `elastic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Instantiate a model and call it `elastic`\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to obtain a cross validated score.\n",
    "\n",
    "Have a look at the cell below, and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_results = cross_val_score(elastic, X_train_preprocessed, y_train, cv=5)\n",
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But we did something wrong! üò±**\n",
    "\n",
    "üëâ Take a moment to think where we made a mistake in the cell above before you read further.\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "When we cross-validated, we only cross-validated the modeling step.\n",
    "\n",
    "In the cross-validation, our training set is split again in a train and a validation set (and that 5 times in our case).\n",
    "\n",
    "But our preprocessing steps were applied to the whole training set, **before** this second split. \n",
    "    \n",
    "This means that we have data leakage between our train and validation set inside the cross-validation.\n",
    "\n",
    "Ideally, for every fold of our cross-validation, we want to:\n",
    "\n",
    "  - Fit all preprocessing steps on the train set\n",
    "  - Transform the train and the validation set\n",
    "  - Train the model on the train\n",
    "  - Score on the validation set\n",
    "\n",
    "The cross-validation we did so far, did not correctly do the preprocessing.\n",
    "</details>\n",
    "\n",
    "üëâ How could we solve this? Take a moment to think about it.\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "We could make a pipeline with all our preprocessing steps + our model, and cross-validate this full pipeline instead of just the model.\n",
    "    \n",
    "That way, the cross-validation will each time fit the preprocessor and the model only on the training set.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Create an `elastic_pipe` by combining your `preprocessing` pipeline with the `elastic` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with your preprocessor and your model\n",
    "# Call it `elastic_pipe`\n",
    "pass  # YOUR CODE HERE\n",
    "\n",
    "# Display elastic_pipe to check your pipeline\n",
    "elastic_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this pipeline in our modelling steps.\n",
    "\n",
    "üëâ Cross-validate `elastic_pipe` and store your average score in `cv_score`\n",
    "\n",
    "Remember we don't need to separately preprocess our training data. Because we combined preprocessing and model into one pipeline, scikit-learn will do the preprocessing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validate it on your training set\n",
    "pass  # YOUR CODE HERE\n",
    "\n",
    "# Save the average score in a variable `cv_score`\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might get a few UserWarnings about unknown categories. That is normal. Do you remember why?\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Answer</i></summary>\n",
    "\n",
    "Remember our OneHotEncoder?\n",
    "\n",
    "It encodes all the categories in the train set into columns with 1s and 0s.\n",
    "    \n",
    "It can happen that in the validation set the encoder encounters categories that it hasn't seen during the fit. In that cases all encoded columns will have 0s (because we chose `handle_unknown='ignore'`.\n",
    "\n",
    "We get multiple warnings: one for each fold where this happens.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('linreg_pipe',\n",
    "    cv_score=cv_score\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our results.\n",
    "\n",
    "What does this `cv_score` represent? What metric are we using here? Are you happy with this score?\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "    For regression models, the default score in scikit-learn is the R<sup>2</sup>.<br>\n",
    "    To know if this is good, we need a baseline.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a baseline model. What would a good baseline be for this task?\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "    For regression models, the simplest thing we can do is to simply use the mean as the prediction.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare our baseline model, we should also cross-validate it, just like our real model. \n",
    "\n",
    "This is where scikit-learn's [dummy models](https://scikit-learn.org/stable/api/sklearn.dummy.html) come in handy. They provide us with a way to use a simple mean calculation as a real scikit-learn model.\n",
    "\n",
    "üëâ Have a look at the cell below, and figure out what we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Create a baseline_pipe using the baseline model\n",
    "baseline_pipe = make_pipeline(\n",
    "    preprocessor,\n",
    "    DummyRegressor(strategy='mean')\n",
    "    )\n",
    "\n",
    "# Cross-validate on the preprocessed training set\n",
    "baseline_cv_results = cross_val_score(baseline_pipe, X_train, y_train, cv=5)\n",
    "\n",
    "# Save the average score in a variable `cv_score`\n",
    "baseline_cv_score = baseline_cv_results.mean()\n",
    "baseline_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is our model performing better than the baseline?\n",
    "\n",
    "üëâ Save your answer \"yes\" or \"no\" in a variable `better_than_baseline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_than_baseline = None # Choose between \"yes\" or \"no\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('baseline_model',\n",
    "    better_than_baseline=better_than_baseline\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are this far, it's time to take a step back.\n",
    "\n",
    "What did we do so far:\n",
    "- We created a function `df_load_and_clean()` to **load our data and do some basic cleaning**\n",
    "- We **split** our data in train and test sets\n",
    "- We created a **preprocessor** using pipelines:\n",
    "  - Imputing missing values\n",
    "  - Scaling numerical features\n",
    "  - Encoding categorical features\n",
    "  - Combine numerical and categorical features back in one big preprocessed DataFrame\n",
    "- We instantiated a **model** and added it to the pipeline\n",
    "- We **cross-validated** the whole pipeline\n",
    "- We compared our model to a **baseline** model (taking the mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-Tuning üéØ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pipelines, we can now start fine-tuning the complete pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In our pipeline we use the ElasticNet model. What are the hyperparameters we can tune? Have a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#elasticnet).\n",
    "<br><br>\n",
    "<details><summary><i>üí°  Solution</i></summary>\n",
    "\n",
    "`alpha`: the amount of regularization we want\n",
    "\n",
    "`l1_ratio`: how much L1 regularization we want (the rest will be L2)\n",
    "</details>\n",
    "\n",
    "üëâ Fine-tune your `elastic_pipe` and store the result in `pipe_grid_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "clear_output"
    ]
   },
   "outputs": [],
   "source": [
    "# First find the name of the correct parameter to tune\n",
    "elastic_pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Let's suppress those UserWarnings we talked about before.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Instantiate a hyperparameter grid with these values:\n",
    "#     'alpha': [0.1, 1.0, 10.0]\n",
    "#     'l1_ratio': [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "pass  # YOUR CODE HERE\n",
    "\n",
    "# Instantiate a GridSearchCV with linreg_pipe and your hyperparameter grid\n",
    "pass  # YOUR CODE HERE\n",
    "\n",
    "# Fit the grid search on X_train, y_train\n",
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Save the score of the best estimator in grid_best_score and display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Save the hyperparameters of the best estimator in grid_best_score and display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Finally, use the best model and score it on the test set, save it into `grid_test_score`\n",
    "\n",
    "Do NOT train a new model, it's not needed: you can get it out of your grid search results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ How many models did the GridSearch train to get to its result?\n",
    "\n",
    "Save your answer in a variable `nb_trainings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('grid_search',\n",
    "    best_params=grid_best_params,\n",
    "    best_score=grid_best_score,\n",
    "    test_score=grid_test_score,\n",
    "    training_count=training_count\n",
    ")\n",
    "result.write()\n",
    "\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How could we take this further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that the grid search at the end was a bit basic.\n",
    "\n",
    "And to be honest: finding better parameters is a bit of a challenge for this model. The models seems to be underfitting.\n",
    "\n",
    "Extra things we could try out:\n",
    "\n",
    "- **Feature engineering**: add extra features that can make your model perform better\n",
    "  Some possibilities:\n",
    "  - `price` divided by `bedrooms`\n",
    "  - Calculate the distance from the city center (which you could approximate by the average of the latitude and longitude of all airbnb's)\n",
    "  - To use those in your pipelines, you'll need to look into custom transformers (check the Workflow lecture)\n",
    "- **Try out other models**: a `KNNRegressor` or an `SVR`\n",
    "- **Grid search (or Randomized search) the hyperparameters of those models** (e.g. `n_neighbours`, `kernel`, `C`)\n",
    "- **Add hyperparameters of the preprocessors to your grid search** (e.g. the imputation strategy)\n",
    "\n",
    "**Don't do this for now, because we will be doing similar things in the next unit! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations! You made it! You did a complete reboot of the Machine Learning workflow!\n",
    "\n",
    "üíæ Don't forget to¬†`git add/commit/push`¬†your notebook..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
